{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QklaM06VPN6v"
      },
      "source": [
        "### Building and training the model\n",
        "\n",
        "Let's start with importing all indispensable libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdspmJuNPN6w"
      },
      "outputs": [],
      "source": [
        "!pip install hazm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import device\n",
        "from tqdm import tqdm_notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchtext\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hazm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKN0jrAHPN6y"
      },
      "source": [
        "Now, we are going to load the tarining and validation sets, but we will use only the clean_review column and label column."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wu5vK1moRytY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/sentipers/final_sentipers_binary.csv', encoding='utf-8')\n",
        "\n",
        "# tokenize data\n",
        "def tokenize(comment):\n",
        "    return hazm.word_tokenize(comment)\n",
        "\n",
        "\n",
        "# creating vocab\n",
        "min_freq = 5\n",
        "data['tokens'] = data['comment'].apply(lambda t: tokenize(t))\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(data['tokens'],\n",
        "                                                  min_freq=min_freq,\n",
        "                                                  specials=special_tokens)\n",
        "unk_index = vocab['<unk>']\n",
        "pad_index = vocab['<pad>']\n",
        "vocab.set_default_index(unk_index)\n",
        "\n",
        "# making input ids\n",
        "# comments are in length between 3 and 256, so we do zero padding for those which have a length less than 256\n",
        "def numeralize(tokens):\n",
        "  ids = [vocab[token] for token in tokens]\n",
        "  ids = np.pad(ids, (0, 256 - len(ids)), 'constant')\n",
        "  return ids\n",
        "\n",
        "# making label ids\n",
        "def toId(label):\n",
        "  return 1 if label=='positive' else 0\n",
        "\n",
        "\n",
        "data['ids']=data['tokens'].apply(lambda t: numeralize(t))\n",
        "data['label']=data['label_id'].apply(lambda t: toId(t))\n",
        "data['length']=data['ids'].apply(lambda t: len(t))\n",
        "\n",
        "data = data[['ids','label','length']]\n",
        "\n",
        "\n",
        "new_data = []\n",
        "for [ids,label,length] in data.values:\n",
        "    new_data.append({'ids':torch.tensor(ids),'label':torch.tensor(label),'length':torch.tensor(length)})"
      ],
      "metadata": {
        "id": "uiZkn2APSAJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukpSXGXrPN65"
      },
      "outputs": [],
      "source": [
        "class BiGRU(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, embedding_dim, output_size, n_layers=1, dropout=0.2,\n",
        "                 spatial_dropout=True, bidirectional=True):\n",
        "        \n",
        "        # Inherit everything from the nn.Module\n",
        "        super(BiGRU, self).__init__()\n",
        "        \n",
        "        # Initialize attributes\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout\n",
        "        self.spatial_dropout = spatial_dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.n_directions = 2 if self.bidirectional else 1\n",
        "        \n",
        "        # Initialize layers\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        if self.spatial_dropout:\n",
        "            self.spatial_dropout1d = nn.Dropout2d(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, num_layers=self.n_layers, \n",
        "                          dropout=(0 if n_layers == 1 else self.dropout_p), batch_first=True,\n",
        "                          bidirectional=self.bidirectional)\n",
        "        # Linear layer input size is equal to hidden_size * 3, becuase\n",
        "        # we will concatenate max_pooling ,avg_pooling and last hidden state\n",
        "        self.linear = nn.Linear(self.hidden_size * 3, self.output_size)\n",
        "\n",
        "        \n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Extract batch_size\n",
        "        self.batch_size = input_seq.size(0)\n",
        "        \n",
        "        # Embeddings shapes\n",
        "        # Input: (batch_size,  seq_length)\n",
        "        # Output: (batch_size, seq_length, embedding_dim)\n",
        "        emb_out = self.embedding(input_seq)\n",
        "\n",
        "        if self.spatial_dropout:\n",
        "            # Convert to (batch_size, embedding_dim, seq_length)\n",
        "            emb_out = emb_out.permute(0, 2, 1)\n",
        "            emb_out = self.spatial_dropout1d(emb_out)\n",
        "            # Convert back to (batch_size, seq_length, embedding_dim)\n",
        "            emb_out = emb_out.permute(0, 2, 1)\n",
        "        else:\n",
        "            emb_out = self.dropout(emb_out)\n",
        "        \n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed_emb = nn.utils.rnn.pack_padded_sequence(emb_out, input_lengths, batch_first=True,enforce_sorted=False)\n",
        "                \n",
        "        # GRU input/output shapes, if batch_first=True\n",
        "        # Input: (batch_size, seq_len, embedding_dim)\n",
        "        # Output: (batch_size, seq_len, hidden_size*num_directions)\n",
        "        gru_out, hidden = self.gru(packed_emb, hidden)\n",
        "        # gru_out: tensor containing the output features h_t from the last layer of the GRU\n",
        "        # gru_out comprises all the hidden states in the last layer (\"last\" depth-wise, not time-wise)\n",
        "        # For biGRu gru_out is the concatenation of a forward GRU representation and a backward GRU representation\n",
        "        # hidden (h_n) comprises the hidden states after the last timestep\n",
        "        \n",
        "        # Extract and sum last hidden state\n",
        "        # Input hidden shape: (n_layers x num_directions, batch_size, hidden_size)\n",
        "        # Separate hidden state layers\n",
        "        hidden = hidden.view(self.n_layers, self.n_directions, self.batch_size, self.hidden_size)\n",
        "        last_hidden = hidden[-1]\n",
        "        # last hidden shape (num_directions, batch_size, hidden_size)\n",
        "        # Sum the last hidden state of forward and backward layer\n",
        "        last_hidden = torch.sum(last_hidden, dim=0)\n",
        "        # Summed last hidden shape (batch_size, hidden_size)\n",
        "        \n",
        "        # Pad a packed batch\n",
        "        # gru_out output shape: (batch_size, seq_len, hidden_size*num_directions)\n",
        "        gru_out, lengths = nn.utils.rnn.pad_packed_sequence(gru_out, batch_first=True)\n",
        "              \n",
        "        # Sum the gru_out along the num_directions\n",
        "        if self.bidirectional:\n",
        "            gru_out = gru_out[:,:,:self.hidden_size] + gru_out[:,:,self.hidden_size:]\n",
        "        \n",
        "        # Output dimensions: (batch_size, hidden_size)\n",
        "        max_pool = F.adaptive_max_pool1d(gru_out.permute(0,2,1), (1,)).view(self.batch_size,-1)\n",
        "        \n",
        "        # Output shape: (batch_size, hidden_size)\n",
        "        avg_pool = torch.sum(gru_out, dim=1) / lengths.view(-1,1).type(torch.FloatTensor) \n",
        "\n",
        "        # Concatenate max_pooling, avg_pooling and last hidden state tensors\n",
        "        concat_out = torch.cat([last_hidden, max_pool, avg_pool], dim=1)\n",
        "\n",
        "        #concat_out = self.dropout(concat_out)\n",
        "        out = self.linear(concat_out)\n",
        "        return F.log_softmax(out, dim=-1)\n",
        "    \n",
        "    \n",
        "    def add_loss_fn(self, loss_fn):\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "\n",
        "    def add_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        \n",
        "    def add_device(self, device=torch.device('cuda')):\n",
        "        self.device = device\n",
        "    \n",
        "    \n",
        "    def train_model(self, train_iterator):\n",
        "        self.train()\n",
        "        \n",
        "        train_losses = []\n",
        "        losses = []\n",
        "        losses_list = []\n",
        "        num_seq = 0\n",
        "        batch_correct = 0\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        for i, batches in tqdm_notebook(enumerate(train_iterator, 1), total=len(train_iterator), desc='Training'):\n",
        "            input_seq, target, x_lengths = batches['ids'].to(device), batches['label'].to(device), batches['length'].to(device)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            pred = self.forward(input_seq, x_lengths)\n",
        "            loss = self.loss_fn(pred, target)\n",
        "            loss.backward()\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            losses_list.append(loss.data.cpu().numpy())\n",
        "            \n",
        "            pred = torch.argmax(pred, 1)\n",
        "\n",
        "            if self.device.type == 'cpu':\n",
        "                batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
        "\n",
        "            else:\n",
        "                batch_correct += (pred == target).sum().item()\n",
        "\n",
        "            num_seq += len(input_seq)     \n",
        "    \n",
        "            if i % 100 == 0:\n",
        "                avg_train_loss = np.mean(losses)\n",
        "                train_losses.append(avg_train_loss)\n",
        "                \n",
        "                accuracy = batch_correct / num_seq\n",
        "                \n",
        "                print('Iteration: {}. Average training loss: {:.4f}. Accuracy: {:.3f}'\\\n",
        "                      .format(i, avg_train_loss, accuracy))\n",
        "                \n",
        "                losses = []\n",
        "                \n",
        "            avg_loss = np.mean(losses_list)\n",
        "            accuracy = batch_correct / num_seq\n",
        "                              \n",
        "        return train_losses, avg_loss, accuracy\n",
        "    \n",
        "    \n",
        "    def evaluate_model(self, eval_iterator, conf_mtx=False):\n",
        "        self.eval()\n",
        "        \n",
        "        eval_losses = []\n",
        "        losses = []\n",
        "        losses_list = []\n",
        "        num_seq = 0\n",
        "        batch_correct = 0\n",
        "        pred_total = torch.LongTensor()\n",
        "        target_total = torch.LongTensor()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        with torch.no_grad():\n",
        "            for i, batches in tqdm_notebook(enumerate(eval_iterator, 1), total=len(eval_iterator), desc='Evaluation'):\n",
        "                input_seq, target, x_lengths = batches['ids'].to(device), batches['label'].to(device), batches['length'].to(device)\n",
        "                \n",
        "                pred = self.forward(input_seq, x_lengths)\n",
        "                loss = self.loss_fn(pred, target)\n",
        "                losses.append(loss.data.cpu().numpy())\n",
        "                losses_list.append(loss.data.cpu().numpy())\n",
        "                \n",
        "                pred = torch.argmax(pred, 1)\n",
        "                                \n",
        "                if self.device.type == 'cpu':\n",
        "                    batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
        "                    \n",
        "                else:\n",
        "                    batch_correct += (pred == target).sum().item()\n",
        "                    \n",
        "                num_seq += len(input_seq)     \n",
        "                \n",
        "                pred_total = torch.cat([pred_total, pred], dim=0)\n",
        "                target_total = torch.cat([target_total, target], dim=0)\n",
        "                \n",
        "                if i % 100 == 0:\n",
        "                    avg_batch_eval_loss = np.mean(losses)\n",
        "                    eval_losses.append(avg_batch_eval_loss)\n",
        "                    \n",
        "                    accuracy = batch_correct / num_seq\n",
        "                    \n",
        "                    print('Iteration: {}. Average evaluation loss: {:.4f}. Accuracy: {:.2f}'\\\n",
        "                          .format(i, avg_batch_eval_loss, accuracy))\n",
        "\n",
        "                    losses = []\n",
        "                    \n",
        "            avg_loss_list = []\n",
        "                    \n",
        "            avg_loss = np.mean(losses_list)\n",
        "            accuracy = batch_correct / num_seq\n",
        "            \n",
        "            conf_matrix = confusion_matrix(target_total.view(-1), pred_total.view(-1))\n",
        "        \n",
        "        if conf_mtx:\n",
        "            print('\\tConfusion matrix: ', conf_matrix)\n",
        "            \n",
        "        return eval_losses, avg_loss, accuracy, conf_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch, pad_index):\n",
        "    batch_ids = [i['ids'] for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = [i['length'] for i in batch]\n",
        "    batch_length = torch.stack(batch_length)\n",
        "    batch_label = [i['label'] for i in batch]\n",
        "    batch_label = torch.stack(batch_label)\n",
        "    batch = {'ids': batch_ids,\n",
        "             'length': batch_length,\n",
        "             'label': batch_label}\n",
        "    return batch"
      ],
      "metadata": {
        "id": "1wXEolqipdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "batch_size = 16\n",
        "collate = functools.partial(collate, pad_index=pad_index)"
      ],
      "metadata": {
        "id": "R_I1TIFwSZbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zal8fZxmPN66"
      },
      "source": [
        "Now we will instantiate the model, add loss function, optimizer, and device to it and begin the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toOsO6jTPN67"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k=5\n",
        "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
        "foldperf={}\n",
        "\n",
        "# Initialize parameters\n",
        "hidden_size = 8\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "output_size = 2\n",
        "learning_rate = 0.001\n",
        "n_layers = 1\n",
        "dropout = 0.5\n",
        "epochs = 10\n",
        "spatial_dropout = True\n",
        "\n",
        "# Check whether system supports CUDA\n",
        "CUDA = torch.cuda.is_available()\n",
        "# Move the model to GPU if possible\n",
        "# if CUDA:\n",
        "#     model.cuda()\n",
        "\n",
        "# fasttext\n",
        "vectors = torchtext.vocab.FastText(language='fa')\n",
        "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the EarlyStopping\n",
        "# early_stop = EarlyStopping(wait_epochs=1)\n",
        "\n",
        "train_losses_list, train_avg_loss_list, train_accuracy_list = [], [], []\n",
        "eval_avg_loss_list, eval_accuracy_list, conf_matrix_list = [], [], []\n",
        "\n",
        "\n",
        "history = {'train_losses': [], 'test_losses': [],'train_accs':[],'test_accs':[]}\n",
        "\n",
        "\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(data)))):\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    test_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "    train_dataloader=torch.utils.data.DataLoader(new_data, batch_size=16,collate_fn=collate, sampler=train_sampler)\n",
        "    test_dataloader=torch.utils.data.DataLoader(new_data, batch_size=16,collate_fn=collate, sampler=test_sampler)\n",
        "        \n",
        "    model = BiGRU(hidden_size, vocab_size, embedding_dim, output_size, n_layers, dropout,\n",
        "              spatial_dropout, bidirectional=False)\n",
        "    \n",
        "    model.embedding.weight.data = pretrained_embedding\n",
        "    model.add_loss_fn(nn.NLLLoss())\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.add_optimizer(optimizer)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.add_device(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        print('\\nStart epoch [{}/{}]'.format(epoch+1, epochs))\n",
        "        \n",
        "        train_losses, train_avg_loss, train_accuracy = model.train_model(train_dataloader)\n",
        "        \n",
        "        train_losses_list.append(train_losses)\n",
        "        train_avg_loss_list.append(train_avg_loss)\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "        \n",
        "        _, eval_avg_loss, eval_accuracy, conf_matrix = model.evaluate_model(test_dataloader)\n",
        "        \n",
        "        eval_avg_loss_list.append(eval_avg_loss)\n",
        "        eval_accuracy_list.append(eval_accuracy)\n",
        "        conf_matrix_list.append(conf_matrix)\n",
        "\n",
        "        history['train_losses'].append(train_avg_loss)\n",
        "        history['test_losses'].append(eval_avg_loss)\n",
        "        history['train_accs'].append(train_accuracy)\n",
        "        history['test_accs'].append(eval_accuracy)\n",
        "        \n",
        "        print('\\nEpoch [{}/{}]: Train accuracy: {:.3f}. Train loss: {:.4f}. Evaluation accuracy: {:.3f}. Evaluation loss: {:.4f}'\\\n",
        "              .format(epoch+1, epochs, train_accuracy, train_avg_loss, eval_accuracy, eval_avg_loss))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average(numList):\n",
        "  return sum(numList)/len(numList)\n",
        "\n",
        "print('accuracy average: ',average(history['test_accs']))\n",
        "print('loss average: ',average(history['test_losses']))"
      ],
      "metadata": {
        "id": "cA1mspZ9BXnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "kfold_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}