{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1797021",
      "metadata": {
        "id": "b1797021"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip install hazm\n",
        "import hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5b5146",
      "metadata": {
        "id": "0d5b5146"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PKZm_edX5k8C"
      },
      "id": "PKZm_edX5k8C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/sentipers/final_sentipers_binary.csv', encoding='utf-8')\n",
        "\n",
        "# tokenize data\n",
        "def tokenize(comment):\n",
        "    return hazm.word_tokenize(comment)\n",
        "\n",
        "\n",
        "# creating vocab\n",
        "min_freq = 5\n",
        "data['tokens'] = data['comment'].apply(lambda t: tokenize(t))\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(data['tokens'],\n",
        "                                                  min_freq=min_freq,\n",
        "                                                  specials=special_tokens)\n",
        "unk_index = vocab['<unk>']\n",
        "pad_index = vocab['<pad>']\n",
        "vocab.set_default_index(unk_index)\n",
        "\n",
        "# making input ids\n",
        "# comments are in length between 3 and 256, so we do zero padding for those which have a length less than 256\n",
        "def numeralize(tokens):\n",
        "  ids = [vocab[token] for token in tokens]\n",
        "  ids = np.pad(ids, (0, 256 - len(ids)), 'constant')\n",
        "  return ids\n",
        "\n",
        "# making label ids\n",
        "def toId(label):\n",
        "  return 1 if label=='positive' else 0\n",
        "\n",
        "\n",
        "data['ids']=data['tokens'].apply(lambda t: numeralize(t))\n",
        "data['label']=data['label_id'].apply(lambda t: toId(t))\n",
        "data['length']=data['ids'].apply(lambda t: len(t))\n",
        "\n",
        "data = data[['ids','label','length']]\n",
        "\n",
        "\n",
        "new_data = []\n",
        "for [ids,label,length] in data.values:\n",
        "    new_data.append({'ids':torch.tensor(ids),'label':torch.tensor(label),'length':torch.tensor(length)})"
      ],
      "metadata": {
        "id": "aOSeVNGI5ack"
      },
      "id": "aOSeVNGI5ack",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cd046f",
      "metadata": {
        "id": "65cd046f"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_rate, \n",
        "                 pad_index):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, \n",
        "                                              n_filters, \n",
        "                                              filter_size,\n",
        "                                             ) \n",
        "                                    for filter_size in filter_sizes])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "    def forward(self, ids):\n",
        "        # ids = [batch size, seq len]\n",
        "        embedded = self.dropout(self.embedding(ids))\n",
        "        # embedded = [batch size, seq len, embedding dim]\n",
        "        embedded = embedded.permute(0,2,1)\n",
        "        # embedded = [batch size, embedding dim, seq len]\n",
        "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
        "        # conved_n = [batch size, n filters, seq len - filter_sizes[n] + 1]\n",
        "        pooled = [conv.max(dim=-1).values for conv in conved]\n",
        "        # pooled_n = [batch size, n filters]\n",
        "        cat = self.dropout(torch.cat(pooled, dim=-1))\n",
        "        # cat = [batch size, n filters * len(filter_sizes)]\n",
        "        prediction = self.fc(cat)\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3da9c4",
      "metadata": {
        "id": "ad3da9c4"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "n_filters = 100\n",
        "filter_sizes = [3,5,7]\n",
        "output_dim = 2\n",
        "dropout_rate = 0.25\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b9314c",
      "metadata": {
        "id": "e5b9314c"
      },
      "outputs": [],
      "source": [
        "# counting params\n",
        "model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_rate, pad_index)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48dd9079",
      "metadata": {
        "id": "48dd9079"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825a973d",
      "metadata": {
        "id": "825a973d"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2216fbd4",
      "metadata": {
        "id": "2216fbd4"
      },
      "outputs": [],
      "source": [
        "def collate(batch, pad_index):\n",
        "    batch_ids = [i['ids'] for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_label = [i['label'] for i in batch]\n",
        "    batch_label = torch.stack(batch_label)\n",
        "    batch = {'ids': batch_ids,\n",
        "             'label': batch_label}\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0513db80",
      "metadata": {
        "id": "0513db80"
      },
      "outputs": [],
      "source": [
        "collate = functools.partial(collate, pad_index=pad_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3651ca7",
      "metadata": {
        "id": "c3651ca7"
      },
      "outputs": [],
      "source": [
        "def train_op(dataloader, model, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    epoch_losses = 0\n",
        "    epoch_accs = 0\n",
        "\n",
        "\n",
        "    for batch in tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        preds = model(ids, length)\n",
        "        loss = criterion(preds, label)\n",
        "        accuracy = get_accuracy(preds, label)\n",
        "        \n",
        "       \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses+=loss.item()\n",
        "        epoch_accs+=accuracy.item()\n",
        "\n",
        "\n",
        "    return epoch_losses/len(dataloader), epoch_accs/len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a96019",
      "metadata": {
        "id": "f2a96019"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model, criterion, device):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_losses = 0\n",
        "    epoch_accs = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            preds = model(ids, length)\n",
        "            loss = criterion(preds, label)\n",
        "            accuracy = get_accuracy(preds, label)\n",
        "            \n",
        "            preds = preds.argmax(dim=-1)\n",
        "            predictions.extend(preds)\n",
        "            labels.extend(label)\n",
        "            epoch_losses+=loss.item()\n",
        "            epoch_accs+=accuracy.item()\n",
        "    \n",
        "    predictions = torch.stack(predictions).cpu().detach().numpy()\n",
        "    labels = torch.stack(labels).cpu().detach().numpy()\n",
        "    f_score = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return epoch_losses/len(dataloader), epoch_accs/len(dataloader) , f_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf2f1e1",
      "metadata": {
        "id": "3cf2f1e1"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing fastText\n",
        "vectors = torchtext.vocab.FastText(language='fa')\n",
        "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
      ],
      "metadata": {
        "id": "Ro8ZU2pd2Yxv"
      },
      "id": "Ro8ZU2pd2Yxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6e8a15",
      "metadata": {
        "id": "af6e8a15"
      },
      "outputs": [],
      "source": [
        "k=5\n",
        "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
        "foldperf={}\n",
        "\n",
        "n_epochs = 10\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "history = {'train_losses': [], 'test_losses': [],'train_accs':[],'test_accs':[],'test_f1s':[]}\n",
        "\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(data)))):\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    test_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "    train_dataloader=torch.utils.data.DataLoader(new_data, batch_size=16,collate_fn=collate, sampler=train_sampler)\n",
        "    test_dataloader=torch.utils.data.DataLoader(new_data, batch_size=16,collate_fn=collate, sampler=test_sampler)\n",
        "        \n",
        "    model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, \n",
        "             pad_index)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    model.embedding.weight.data = pretrained_embedding\n",
        "\n",
        "    lr = 5e-4\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train_op(train_dataloader, model, criterion, optimizer, device)\n",
        "        test_loss, test_acc , test_f1 = evaluate(test_dataloader, model, criterion, device)\n",
        "\n",
        "        \n",
        "        history['train_losses'].append(train_loss)\n",
        "        history['test_losses'].append(test_loss)\n",
        "        history['train_accs'].append(train_acc)\n",
        "        history['test_accs'].append(test_acc)\n",
        "        history['test_f1s'].append(test_f1)\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}')\n",
        "        print(f'valid_loss: {test_loss:.3f}, valid_acc: {test_acc:.3f}')\n",
        "\n",
        "    foldperf['fold{}'.format(fold+1)] = history  \n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'bilstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average(numList):\n",
        "  return sum(numList)/len(numList)\n",
        "\n",
        "print('accuracy average: ',average(history['test_accs']))\n",
        "print('loss average: ',average(history['test_losses']))\n",
        "print('f1 average: ',average(history['test_f1s']))"
      ],
      "metadata": {
        "id": "ovrJOXKSeIDA"
      },
      "id": "ovrJOXKSeIDA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "kfold_cnn_fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}